# What is Attention

Attention is a mechanism for focusing on relevant parts of the input when making predictions.
Instead of treating all input tokens equally, attention asks:
