# What is Attention

Attention is a mechanism for focusing on relevant parts of the input when making predictions.
Instead of treating all input tokens equally, attention asks:

👉 “When I process this word (or token), which other words are most important for understanding it?”

## The core math behind attention is Scaled dot product

We have three matrices

1. Query
