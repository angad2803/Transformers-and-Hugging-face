# What is Attention

Attention is a mechanism for focusing on relevant parts of the input when making predictions.
Instead of treating all input tokens equally, attention asks:

ğŸ‘‰ â€œWhen I process this word (or token), which other words are most important for understanding it?â€

## The core math behind attention is Scaled dot product

We have three matrices

1. Query
