# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.

Bidirectional Encoder Representation of Transformers has become one of the most preferred choice of language model for fine-tuning across different downstream tasks

![alt text](image-4.png)

![alt text](image-5.png)
