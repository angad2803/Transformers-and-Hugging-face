# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.

Bidirectional Encoder Representation of Transformers has become one of the most preferred choice of language model for fine-tuning across different downstream tasks

![alt text](image-4.png)
Every input token is represented as:

# InputEmbedding

TokenEmbedding

- SegmentEmbedding
- PositionEmbedding
  InputEmbedding=TokenEmbedding+SegmentEmbedding+PositionEmbedding

Token embeddings = learned vectors for each word-piece (WordPiece tokenizer).

Segment embeddings = distinguish sentence A vs. sentence B (needed for NSP).

Position embeddings = since Transformer has no recurrence, we add position info.
![alt text](image-5.png)
