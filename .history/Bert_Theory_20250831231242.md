# BERT: Full Explanation (Devlin et al., 2018)

BERT uses an encoder-only Transformer architecture focused on understanding text.

## Encoder Block Structure

- Multi-head self-attention
- Feedforward neural network (FFNN)
- Add & Norm (residual connections)

**Why encoder-only?**
- Decoders (for generation) are not needed; BERT is for comprehension.
- Bidirectional self-attention enables rich representation learning.

BERT (Bidirectional Encoder Representation from Transformers) is widely fine-tuned for many NLP tasks.

![BERT architecture](image-4.png)

## Input Embedding

Each token’s input embedding is the sum of:
- **Token Embedding:** Learned for each word-piece (WordPiece tokenizer)
- **Segment Embedding:** Distinguishes sentence A from B (for NSP)
- **Position Embedding:** Adds positional info (since Transformers lack recurrence)

`InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding`

![Input embedding](image-5.png)

## Pretraining Tasks

### 1. Masked Language Modeling (MLM)

**Goal:** Predict randomly masked tokens using bidirectional context.

- Randomly mask

**How it works:**

- Randomly select 15% of tokens in each input sequence.
  - 80% replaced with `[MASK]`
  - 10% replaced with a random token
  - 10% remain unchanged
- The model predicts the original token at each masked position.

**Why it matters:**  
MLM enables BERT to use context from both directions, helping it learn nuanced meanings (e.g., “bank” in different contexts).

### 2. Next Sentence Prediction (NSP)

**Task:** Given two sentences, predict if the second follows the first in the corpus.

- 50% of the time, the second sentence is the true next sentence.
- 50% of the time, it is a random sentence.
- The model predicts IsNext / NotNext.

**Purpose:**  
NSP teaches BERT about sentence-level coherence.

Useful for QA, NLI, entailment, where relationships across sentences matter.

✅ Pretraining Output: A model with deep bidirectional contextual embeddings.

4. Fine-tuning

Once pretrained, BERT can be adapted for any supervised NLP task by adding a small layer on top.

How it works:

Input representation = same as pretraining.

Entire BERT is fine-tuned (not frozen) with task-specific labeled data.

Only the final output layer changes per task.

a) Single sentence classification (e.g., sentiment)

Input: [CLS] sentence [SEP]

Output: Use [CLS] embedding → softmax classifier.

b) Sentence pair tasks (e.g., NLI, QA pair)

Input: [CLS] sentence_A [SEP] sentence_B [SEP]

Segment embeddings distinguish A vs B.

[CLS] embedding → softmax for entailment / similarity.

c) Token-level tasks (e.g., NER, POS tagging)

Input: [CLS] sentence [SEP]

Output: Each token embedding → classification head (predict tag per word).

d) Question Answering (e.g., SQuAD)

Input: [CLS] question [SEP] passage [SEP]

Output: Two classifiers predict start and end positions of the answer span.

5. Training Passes (How It Actually Runs)

Let’s simulate a pass:

Pretraining Pass (MLM)

Input: “The cat sat on the [MASK].”

BERT encoder computes contextual embeddings for all tokens.

At [MASK], output hidden state is fed into classification layer.

Softmax over vocab → predict “mat”.

Loss = cross-entropy between predicted token distribution & true word.

Pretraining Pass (NSP)

Input: “[CLS] The cat sat on the mat. [SEP] It was tired. [SEP]”

[CLS] embedding → linear layer → sigmoid (IsNext / NotNext).

Loss = binary cross-entropy.

Fine-tuning Pass (e.g., sentiment classification)

Input: “[CLS] I loved the movie. [SEP]”

[CLS] embedding → classification layer → Softmax (Positive/Negative).

Loss = cross-entropy.
