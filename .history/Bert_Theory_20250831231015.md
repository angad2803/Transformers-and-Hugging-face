# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.

Bidirectional Encoder Representation of Transformers has become one of the most preferred choice of language model for fine-tuning across different downstream tasks

![alt text](image-4.png)
Every input token is represented as:

## InputEmbedding

**Input Embedding Components:**

- **Token Embedding:** Learned vectors for each word-piece (using WordPiece tokenizer).
- **Segment Embedding:** Distinguishes between sentence A and sentence B (used for Next Sentence Prediction).
- **Position Embedding:** Adds positional information since Transformers lack recurrence.

**InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding**

![alt text](image-5.png)

## Pretraining (Two Self-Supervised Tasks)

a) Masked Language Modeling (MLM)
a) Masked Language Modeling (MLM)

Idea: Predict randomly masked tokens → allows bidirectional context (both left & right).

Steps:

Pick 15% of tokens.

For each:

80% replaced with [MASK] (e.g., “the dog chased the [MASK]”).

10% replaced with random token.

10% left unchanged.
(This keeps the task from becoming too artificial).

Model predicts the original token at those positions.

Why this works:

Unlike GPT (left-to-right), MLM forces model to look at all words before and after the masked word.

Example: In “Bank of the river” vs “Bank of America” → “bank” has different meanings. MLM forces model to learn context.
