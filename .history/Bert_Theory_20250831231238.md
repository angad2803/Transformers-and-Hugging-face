# BERT: Full Explanation (Devlin et al., 2018)

BERT is an encoder-only Transformer architecture.

**Each encoder block consists of:**

- Multi-head self-attention
- Feedforward neural network (FFNN)
- Add & Norm (with residual connections)

**Why encoder-only?**

- Decoders (used in translation) are not needed since BERT focuses on understanding, not generating text.
- Bidirectional self-attention in the encoder is ideal for representation learning.

BERT (Bidirectional Encoder Representation from Transformers) is widely used for fine-tuning on various downstream NLP tasks.

![alt text](image-4.png)

## Input Embedding

Each input token is represented by a combination of embeddings:

- **Token Embedding:** Learned vectors for each word-piece (WordPiece tokenizer).
- **Segment Embedding:** Distinguishes sentence A from sentence B (used in Next Sentence Prediction).
- **Position Embedding:** Adds positional information (since Transformers lack recurrence).

**InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding**

![alt text](image-5.png)

## Pretraining (Two Self-Supervised Tasks)

### 1. Masked Language Modeling (MLM)

**Goal:** Predict randomly masked tokens using bidirectional context.

**How it works:**

- Randomly select 15% of tokens in each input sequence.
  - 80% replaced with `[MASK]`
  - 10% replaced with a random token
  - 10% remain unchanged
- The model predicts the original token at each masked position.

**Why it matters:**  
MLM enables BERT to use context from both directions, helping it learn nuanced meanings (e.g., “bank” in different contexts).

### 2. Next Sentence Prediction (NSP)

**Task:** Given two sentences, predict if the second follows the first in the corpus.

- 50% of the time, the second sentence is the true next sentence.
- 50% of the time, it is a random sentence.
- The model predicts IsNext / NotNext.

**Purpose:**  
NSP teaches BERT about sentence-level coherence.

Useful for QA, NLI, entailment, where relationships across sentences matter.

✅ Pretraining Output: A model with deep bidirectional contextual embeddings.

4. Fine-tuning

Once pretrained, BERT can be adapted for any supervised NLP task by adding a small layer on top.

How it works:

Input representation = same as pretraining.

Entire BERT is fine-tuned (not frozen) with task-specific labeled data.

Only the final output layer changes per task.

a) Single sentence classification (e.g., sentiment)

Input: [CLS] sentence [SEP]

Output: Use [CLS] embedding → softmax classifier.

b) Sentence pair tasks (e.g., NLI, QA pair)

Input: [CLS] sentence_A [SEP] sentence_B [SEP]

Segment embeddings distinguish A vs B.

[CLS] embedding → softmax for entailment / similarity.

c) Token-level tasks (e.g., NER, POS tagging)

Input: [CLS] sentence [SEP]

Output: Each token embedding → classification head (predict tag per word).

d) Question Answering (e.g., SQuAD)

Input: [CLS] question [SEP] passage [SEP]

Output: Two classifiers predict start and end positions of the answer span.

5. Training Passes (How It Actually Runs)

Let’s simulate a pass:

Pretraining Pass (MLM)

Input: “The cat sat on the [MASK].”

BERT encoder computes contextual embeddings for all tokens.

At [MASK], output hidden state is fed into classification layer.

Softmax over vocab → predict “mat”.

Loss = cross-entropy between predicted token distribution & true word.

Pretraining Pass (NSP)

Input: “[CLS] The cat sat on the mat. [SEP] It was tired. [SEP]”

[CLS] embedding → linear layer → sigmoid (IsNext / NotNext).

Loss = binary cross-entropy.

Fine-tuning Pass (e.g., sentiment classification)

Input: “[CLS] I loved the movie. [SEP]”

[CLS] embedding → classification layer → Softmax (Positive/Negative).

Loss = cross-entropy.
