# BERT: Full Explanation (Devlin et al., 2018)

BERT is an encoder-only Transformer architecture.

**Each encoder block consists of:**

- Multi-head self-attention
- Feedforward neural network (FFNN)
- Add & Norm (with residual connections)

**Why encoder-only?**

- Decoders (used in translation) are not needed since BERT focuses on understanding, not generating text.
- Bidirectional self-attention in the encoder is ideal for representation learning.

BERT (Bidirectional Encoder Representation from Transformers) is widely used for fine-tuning on various downstream NLP tasks.

![alt text](image-4.png)

## Input Embedding

Each input token is represented by a combination of embeddings:

- **Token Embedding:** Learned vectors for each word-piece (WordPiece tokenizer).
- **Segment Embedding:** Distinguishes sentence A from sentence B (used in Next Sentence Prediction).
- **Position Embedding:** Adds positional information (since Transformers lack recurrence).

**InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding**

![alt text](image-5.png)

## Pretraining (Two Self-Supervised Tasks)

### 1. Masked Language Modeling (MLM)

**Goal:** Predict randomly masked tokens using bidirectional context.

**How it works:**

- Randomly select 15% of tokens in each input sequence.
  - 80% replaced with `[MASK]`
  - 10% replaced with a random token
  - 10% remain unchanged
- The model predicts the original token at each masked position.

**Why it matters:**  
MLM enables BERT to use context from both directions, helping it learn nuanced meanings (e.g., “bank” in different contexts).

### 2. Next Sentence Prediction (NSP)

**Task:** Given two sentences, predict if the second follows the first in the corpus.

- 50% of the time, the second sentence is the true next sentence.
- 50% of the time, it is a random sentence.
- The model predicts IsNext / NotNext.

**Purpose:**  
NSP teaches BERT about sentence-level coherence.

Useful for QA, NLI, entailment, where relationships across sentences matter.

✅ Pretraining Output: A model with deep bidirectional contextual embeddings.
