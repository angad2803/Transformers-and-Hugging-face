The Problem- Attention has no sense of order.

Self-attention treats a sequence as a set â†’ it only cares about relationships (QÂ·K similarities), not where tokens appear.

Example: â€œBank near the riverâ€ vs â€œRiver near the bankâ€.

Same words, but meaning depends on word order.

So we need a way to inject position information into embeddings.

![alt text](image-3.png)

## What "Attention is All You Need" Did

They introduced sinusoidal positional encoding added to word embeddings.

For each position pos and dimension i:

ğ‘ƒ
ğ¸
(
ğ‘
ğ‘œ
ğ‘ 
,
2
ğ‘–
)
=
sin
â¡
(
ğ‘
ğ‘œ
ğ‘ 
10000
2
ğ‘–
/
ğ‘‘
ğ‘š
ğ‘œ
ğ‘‘
ğ‘’
ğ‘™
)
PE
(pos,2i)
â€‹

=sin(
10000
2i/d
model
â€‹

pos
â€‹

)
ğ‘ƒ
ğ¸
(
ğ‘
ğ‘œ
ğ‘ 
,
2
ğ‘–

- 1
  )
  =
  cos
  â¡
  (
  ğ‘
  ğ‘œ
  ğ‘ 
  10000
  2
  ğ‘–
  /
  ğ‘‘
  ğ‘š
  ğ‘œ
  ğ‘‘
  ğ‘’
  ğ‘™
  )
  PE
  (pos,2i+1)
  â€‹

=cos(
10000
2i/d
model
â€‹

pos
â€‹

)

This way:

Low-frequency sinusoids capture global order (long-range).

High-frequency sinusoids capture local order (nearby words).

Since sine/cosine are periodic, the encoding generalizes to unseen sequence lengths.
