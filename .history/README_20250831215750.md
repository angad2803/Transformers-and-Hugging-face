## Summary of Contents

- **Theoretical Part:**
  - Explains the concept of positional encoding, its importance in transformer models, and how it enables models to understand word order in sequences.
  - Related markdown files: `Attention_Theory.md`, `Positional_Encoding.md`, `Transformers_theory.md`
- Mathematical formulation and implementation of positional encoding, with code and visualizations to illustrate how it works.
- **BERT Fine-Tune Code:**
  - Example code for fine-tuning a BERT model using Hugging Face's Transformers library, showing how positional encoding is applied in real NLP tasks.
