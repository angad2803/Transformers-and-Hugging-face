# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.
