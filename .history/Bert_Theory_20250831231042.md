# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.

Bidirectional Encoder Representation of Transformers has become one of the most preferred choice of language model for fine-tuning across different downstream tasks

![alt text](image-4.png)
Every input token is represented as:

## InputEmbedding

**Input Embedding Components:**

- **Token Embedding:** Learned vectors for each word-piece (using WordPiece tokenizer).
- **Segment Embedding:** Distinguishes between sentence A and sentence B (used for Next Sentence Prediction).
- **Position Embedding:** Adds positional information since Transformers lack recurrence.

**InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding**

![alt text](image-5.png)

## Pretraining (Two Self-Supervised Tasks)

### a) Masked Language Modeling (MLM)

**Idea:** Predict randomly masked tokens to enable bidirectional context (using both left and right surroundings).

**How it works:**

- Select 15% of tokens in each input sequence.
- For each selected token:

  - 80% are replaced with `[MASK]` (e.g., “the dog chased the [MASK]”).
  - 10% are replaced with a random token.
  - 10% remain unchanged.
  - (This prevents the task from being too artificial.)

- The model is trained to predict the original token at each masked position.

**Why it matters:**  
Unlike left-to-right models (e.g., GPT), MLM forces BERT to use context from both directions.  
For example, in “Bank of the river” vs “Bank of America”, the meaning of “bank” depends on context. MLM helps the model learn these distinctions.

b) Next Sentence Prediction (NSP)

Task: Given two sentences, decide if B follows A in the corpus.

Steps:

50% of the time → B is the true next sentence.

50% of the time → B is a random sentence.

Model predicts IsNext / NotNext.

Why this works:

Teaches model about sentence-level coherence.

Useful for QA, NLI, entailment, where relationships across sentences matter.
