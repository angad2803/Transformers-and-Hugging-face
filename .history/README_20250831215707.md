## Summary of Contents

- **Theoretical Part:**
  - Explains the concept of positional encoding, its importance in transformer models, and how it enables models to understand word order in sequences.
- **Positional Encoding:**
  - Mathematical formulation and implementation of positional encoding, with code and visualizations to illustrate how it works.
- **BERT Fine-Tune Code:**
  - Example code for fine-tuning a BERT model using Hugging Face's Transformers library, showing how positional encoding is applied in real NLP tasks.
