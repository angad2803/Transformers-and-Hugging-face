# BERT: Full Explanation (Devlin et al., 2018)

BERT is encoder-only Transformer:

Each encoder block =

Multi-head self-attention

Feedforward neural net (FFNN)

Add & Norm + residuals

Why encoder-only?

Decoder (used in translation) isn’t needed since BERT isn’t generating text — it’s understanding text.

The encoder’s bidirectional self-attention fits “representation learning” best.

Bidirectional Encoder Representation of Transformers has become one of the most preferred choice of language model for fine-tuning across different downstream tasks

![alt text](image-4.png)
Every input token is represented as:

## InputEmbedding

**Input Embedding Components:**

- **Token Embedding:** Learned vectors for each word-piece (using WordPiece tokenizer).
- **Segment Embedding:** Distinguishes between sentence A and sentence B (used for Next Sentence Prediction).
- **Position Embedding:** Adds positional information since Transformers lack recurrence.

**InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding**

![alt text](image-5.png)

## Pretraining (Two Self-Supervised Tasks)
